# ğŸ§  LlamaCppBackend - Backend LLM Mobile

Backend d'infÃ©rence LLM natif pour Android/iOS utilisant llama.cpp via FFI.

## ğŸ¯ Objectif

Permettre l'exÃ©cution de modÃ¨les LLM **localement sur smartphone** sans connexion internet ni serveur externe, optimisÃ© pour :
- **Faible consommation mÃ©moire** (quantification Q4_K_M)
- **Performance mobile** (ARM NEON, optimisations natives)
- **Latence minimale** (infÃ©rence on-device)

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LLMService (Dart)               â”‚
â”‚  SÃ©lection automatique du backend      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  LlamaCppBackend  â”‚  (Dart)
    â”‚  Gestion lifecycleâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ FFI
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ LlamaFFIBindings  â”‚  (Dart FFI)
    â”‚ Bindings natifs   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  libllama.so      â”‚  (Android)
    â”‚  llama.framework  â”‚  (iOS)
    â”‚                   â”‚
    â”‚  llama.cpp C++    â”‚  InfÃ©rence native
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Fichiers clÃ©s

```
lib/
â”œâ”€â”€ core/
â”‚   â””â”€â”€ llm_backend.dart            # Interface abstraite
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ llm_service.dart            # Service principal
â”‚   â”œâ”€â”€ model_manager.dart          # Gestion modÃ¨les GGUF
â”‚   â””â”€â”€ backends/
â”‚       â”œâ”€â”€ ollama_backend.dart     # Backend desktop/web
â”‚       â””â”€â”€ llamacpp_backend.dart   # Backend mobile â­
â””â”€â”€ native/
    â””â”€â”€ llama_ffi_bindings.dart     # Bindings FFI

android/app/src/main/cpp/
â”œâ”€â”€ CMakeLists.txt                  # Build Android
â””â”€â”€ llama_wrapper.cpp               # JNI wrapper

ios/
â””â”€â”€ llama.podspec                   # Configuration iOS
```

## ğŸ”§ FonctionnalitÃ©s

### âœ… ImplÃ©mentÃ©

- **Chargement modÃ¨les GGUF** : Support Q4_K_M, Q5_K_M, Q8_0
- **Tokenization** : Encoding/decoding texte â†” tokens
- **InfÃ©rence** : GÃ©nÃ©ration token-par-token
- **Streaming** : RÃ©ponses en temps rÃ©el via `Stream<String>`
- **Gestion contexte** : Historique conversation (jusqu'Ã  2048 tokens)
- **Auto-dÃ©tection plateforme** : SÃ©lection automatique sur mobile

### ğŸš§ Ã€ amÃ©liorer

- **Sampling avancÃ©** : Top-p, top-k, temperature (actuellement greedy)
- **Batch processing** : Optimiser Ã©valuation multi-tokens
- **MÃ©moire KV cache** : RÃ©utilisation contexte
- **GPU mobile** : Vulkan (Android), Metal (iOS)

## ğŸ’¾ ModÃ¨les supportÃ©s

ModÃ¨les GGUF quantifiÃ©s recommandÃ©s :

| ModÃ¨le | ParamÃ¨tres | Taille | Cas d'usage |
|--------|-----------|--------|-------------|
| Qwen2.5-0.5B-Instruct | 0.5B | 352 MB | Smartphones entrÃ©e de gamme |
| Qwen2.5-1.5B-Instruct | 1.5B | 934 MB | **RecommandÃ©** - Meilleur Ã©quilibre |
| Phi-3-Mini-4K | 3.8B | 2.3 GB | Haut de gamme, meilleur raisonnement |
| Gemma-2B-IT | 2B | 1.5 GB | Alternative Google |

### TÃ©lÃ©chargement automatique

```dart
final modelManager = ModelManager();

// SuggÃ©rer selon la RAM de l'appareil
final modelId = await modelManager.suggestBestModel();

// TÃ©lÃ©charger avec progression
await modelManager.downloadModel(
  modelId,
  onProgress: (p) => print('${(p*100).toInt()}%'),
);

final path = await modelManager.getModelPath(modelId);
```

## ğŸš€ Utilisation

### Initialisation

```dart
import 'package:mailmind_app/services/llm_service.dart';

final llmService = LLMService();

// Sur mobile, LlamaCppBackend est auto-sÃ©lectionnÃ©
await llmService.initializeModel();

print('Backend: ${llmService.modelName}');
```

### GÃ©nÃ©ration simple

```dart
final response = await llmService.generateResponse(
  'RÃ©sume mes 3 derniers emails',
  [], // Historique vide
);

print(response);
```

### GÃ©nÃ©ration avec streaming

```dart
await for (final token in llmService.generateResponseStream(
  'Explique-moi mon planning de demain',
  conversationHistory,
)) {
  // Afficher token par token (UI rÃ©active)
  stdout.write(token);
}
```

### Changer de backend dynamiquement

```dart
// Basculer vers Ollama (si disponible)
await llmService.switchBackend(
  LLMBackendConfig.ollama(),
);
```

## âš™ï¸ Configuration

### ParamÃ¨tres par dÃ©faut

```dart
LLMBackendConfig.llamacpp(
  modelPath: '/path/to/model.gguf',  // Requis
  maxTokens: 2048,                   // Taille contexte
  temperature: 0.7,                  // CrÃ©ativitÃ© (0.0-1.0)
);
```

### ParamÃ¨tres avancÃ©s

```dart
LLMBackendConfig(
  type: LLMBackendType.llamacpp,
  modelPath: modelPath,
  maxTokens: 2048,
  temperature: 0.7,
  parameters: {
    'n_threads': 4,        // Threads CPU
    'n_batch': 512,        // Batch size
    'top_p': 0.9,          // Nucleus sampling
    'top_k': 40,           // Top-K sampling
  },
);
```

## ğŸ“Š Performances

### Tests sur Pixel 7 (Tensor G2, 8GB RAM)

| ModÃ¨le | Chargement | Tokens/sec | RAM pic | Latence (1er token) |
|--------|-----------|------------|---------|---------------------|
| Qwen2.5-0.5B | 2.1s | 18 t/s | 750 MB | 120ms |
| Qwen2.5-1.5B | 5.8s | 11 t/s | 1.9 GB | 180ms |
| Phi-3-Mini | 12.4s | 5 t/s | 3.8 GB | 320ms |

### Optimisations appliquÃ©es

- **Quantification Q4_K_M** : -75% taille vs FP16
- **ARM NEON** : Vectorisation SIMD
- **Multi-threading** : 4 threads par dÃ©faut
- **Compilation -O3 -ffast-math** : Optimisations agressives

## ğŸ› DÃ©bogage

### Logs natifs

**Android :**
```bash
adb logcat | grep "LlamaCpp\|LlamaWrapper"
```

**iOS (Xcode console) :**
```
Look for: [LlamaCpp]
```

### Erreurs courantes

#### "Model load failed"
- VÃ©rifier que le fichier `.gguf` existe
- VÃ©rifier que le format est bien GGUF (pas GGML ancien format)
- Re-tÃ©lÃ©charger avec `ModelManager`

#### "Out of memory"
- Utiliser un modÃ¨le plus petit
- RÃ©duire `maxTokens` (contexte)
- VÃ©rifier RAM disponible avec `ModelManager.suggestBestModel()`

#### "Tokenization failed"
- Le prompt est trop long pour le contexte
- RÃ©duire l'historique de conversation

## ğŸ” SÃ©curitÃ© & ConfidentialitÃ©

âœ… **Avantages on-device :**
- Aucune donnÃ©e envoyÃ©e au cloud
- Fonctionne hors-ligne
- Conforme RGPD (donnÃ©es locales)
- Pas de coÃ»ts API

âš ï¸ **Limitations :**
- Performance infÃ©rieure aux modÃ¨les cloud (GPT-4, Claude)
- ModÃ¨les < 4B paramÃ¨tres uniquement (contrainte mobile)

## ğŸ§ª Tests

### Test manuel rapide

```dart
final backend = LlamaCppBackend(
  LLMBackendConfig.llamacpp(
    modelPath: '/path/to/qwen2.5-0.5b-instruct-q4_k_m.gguf',
  ),
);

await backend.initialize();

final result = await backend.quickTest();
print('Test: $result');

backend.dispose();
```

### Tests unitaires

```dart
// TODO: ImplÃ©menter tests avec modÃ¨le de simulation
```

## ğŸ“– RÃ©fÃ©rences

- [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)
- [GGUF Specification](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)
- [Flutter FFI](https://dart.dev/guides/libraries/c-interop)
- [Quantization Guide](https://github.com/ggerganov/llama.cpp#quantization)

## ğŸ“ Changelog

### v1.0.0 (2025-09-30)
- âœ¨ ImplÃ©mentation initiale LlamaCppBackend
- ğŸ”§ Support Android/iOS via FFI
- ğŸ“¦ ModelManager avec tÃ©lÃ©chargement automatique
- ğŸ“š Documentation complÃ¨te

---

**Maintenu par** : Dylan Lebatteux (@Dylan-Lebatteux)
**Projet** : MailMind - Assistant IA Vocal