import 'dart:async';
import 'dart:io';
import '../core/llm_backend.dart';
import '../services/backends/ollama_backend.dart';
import '../models/conversation_context.dart';
import '../models/message.dart';

/// Service principal de gestion LLM avec architecture multi-backend
class LLMService {
  static final LLMService _instance = LLMService._internal();
  factory LLMService() => _instance;
  LLMService._internal();

  // Backend actuel
  LLMBackend? _currentBackend;
  ConversationContext? _currentContext;

  // Configuration
  LLMBackendConfig? _config;
  bool _isInitialized = false;

  // Stream controllers pour compatibilit√© avec l'UI existante
  final StreamController<LLMStatus> _statusController = StreamController.broadcast();
  final StreamController<String> _responseStreamController = StreamController.broadcast();

  // Getters pour compatibilit√© avec l'interface existante
  LLMStatus get status => _currentBackend?.status ?? LLMStatus.notInitialized;
  String get errorMessage => _currentBackend?.errorMessage ?? '';
  double get downloadProgress => _currentBackend?.downloadProgress ?? 0.0;
  bool get isReady => _currentBackend?.isReady ?? false;
  String get modelName => _currentBackend?.modelName ?? 'unknown';

  Stream<LLMStatus> get statusStream => _statusController.stream;
  Stream<String> get responseStream => _responseStreamController.stream;

  /// Initialiser le service avec d√©tection automatique du backend optimal
  Future<void> initializeModel() async {
    if (_isInitialized) return;

    try {
      print('üöÄ Initialisation LLM Service avec architecture multi-backend');

      // D√©tecter la configuration optimale pour la plateforme
      _config = _detectOptimalBackend();
      print('üéØ Backend s√©lectionn√©: ${_config!.type}');

      // Cr√©er le backend appropri√©
      _currentBackend = _createBackend(_config!);

      // S'abonner aux changements de status du backend
      _currentBackend!.statusStream.listen((status) {
        _statusController.add(status);
      });

      // Initialiser le backend
      await _currentBackend!.initialize();

      _isInitialized = true;
      print('‚úÖ LLM Service initialis√© avec succ√®s');

    } catch (e) {
      print('‚ùå Erreur initialisation LLM Service: $e');
      _statusController.add(LLMStatus.error);
      rethrow;
    }
  }

  /// Initialiser avec une configuration sp√©cifique
  Future<void> initializeWithConfig(LLMBackendConfig config) async {
    _config = config;
    await initializeModel();
  }

  /// G√©n√©rer une r√©ponse
  Future<String> generateResponse(
    String userMessage,
    List<String> conversationHistory
  ) async {
    if (_currentBackend == null || !_currentBackend!.isReady) {
      throw Exception('LLM Service non pr√™t');
    }

    try {
      // Convertir l'historique en contexte si n√©cessaire
      ConversationContext? context;
      if (conversationHistory.isNotEmpty) {
        context = _buildContextFromHistory(conversationHistory);
      } else {
        context = _currentContext;
      }

      // G√©n√©rer avec le backend actuel
      final response = await _currentBackend!.generateResponse(userMessage, context);

      // Mettre √† jour le contexte de conversation
      _updateConversationContext(userMessage, response);

      return response;

    } catch (e) {
      print('‚ùå Erreur g√©n√©ration: $e');
      throw Exception('Erreur de g√©n√©ration: $e');
    }
  }

  /// G√©n√©rer une r√©ponse en streaming
  Stream<String> generateResponseStream(
    String userMessage,
    List<String> conversationHistory
  ) async* {
    if (_currentBackend == null || !_currentBackend!.isReady) {
      throw Exception('LLM Service non pr√™t');
    }

    ConversationContext? context;
    if (conversationHistory.isNotEmpty) {
      context = _buildContextFromHistory(conversationHistory);
    } else {
      context = _currentContext;
    }

    await for (final token in _currentBackend!.generateResponseStream(userMessage, context)) {
      yield token;
    }
  }

  /// V√©rifier l'initialisation (pour compatibilit√©)
  Future<void> checkInitialization() async {
    _isInitialized = false;
    _statusController.add(LLMStatus.notInitialized);
  }

  /// Obtenir des informations sur le mod√®le
  Future<String> getModelInfo() async {
    if (_currentBackend == null) {
      return "Backend: non initialis√©";
    }

    try {
      final info = await _currentBackend!.getModelInfo();
      final buffer = StringBuffer();
      buffer.writeln('=== MailMind LLM Service ===');
      buffer.writeln('Backend: ${info['backend']}');
      buffer.writeln('Mod√®le: ${info['model']}');
      buffer.writeln('Status: ${info['status']}');
      buffer.writeln('Plateforme: ${info['platform']}');
      if (info['server_url'] != null) {
        buffer.writeln('Serveur: ${info['server_url']}');
      }
      buffer.writeln('Disponible: ${info['available']}');
      return buffer.toString();
    } catch (e) {
      return "Erreur r√©cup√©ration info: $e";
    }
  }

  /// D√©marrer une nouvelle conversation
  void startNewConversation() {
    _currentContext = ConversationContext(
      messages: [],
      sessionId: DateTime.now().millisecondsSinceEpoch.toString(),
      startTime: DateTime.now(),
    );
    print('üîÑ Nouvelle conversation d√©marr√©e: ${_currentContext!.sessionId}');
  }

  /// Changer de backend dynamiquement
  Future<void> switchBackend(LLMBackendConfig newConfig) async {
    print('üîÑ Changement de backend: ${_config?.type} ‚Üí ${newConfig.type}');

    // Lib√©rer l'ancien backend
    _currentBackend?.dispose();

    // Cr√©er et initialiser le nouveau
    _config = newConfig;
    _currentBackend = _createBackend(newConfig);

    _currentBackend!.statusStream.listen((status) {
      _statusController.add(status);
    });

    await _currentBackend!.initialize();
    print('‚úÖ Backend chang√© avec succ√®s');
  }

  /// Nettoyer les ressources
  void dispose() {
    _currentBackend?.dispose();
    _statusController.close();
    _responseStreamController.close();
    print('üóëÔ∏è LLM Service lib√©r√©');
  }

  // === M√©thodes priv√©es ===

  /// D√©tecter le backend optimal selon la plateforme
  LLMBackendConfig _detectOptimalBackend() {
    if (_isMobilePlatform()) {
      print('üì± Plateforme mobile d√©tect√©e - Ollama Backend s√©lectionn√©');
      // Sur √©mulateur Android, 10.0.2.2 pointe vers le PC h√¥te
      // Sur device physique ou iOS, utilisez l'IP locale de votre PC
      return LLMBackendConfig.ollama(
        serverUrl: 'http://10.0.2.2:11434',
        model: 'qwen2.5:1.5b',
      );
    } else if (_isWebPlatform()) {
      print('üåê Plateforme web d√©tect√©e - Ollama Backend s√©lectionn√©');
      return LLMBackendConfig.ollama();
    } else {
      print('üñ•Ô∏è Plateforme desktop d√©tect√©e - Ollama Backend s√©lectionn√©');
      return LLMBackendConfig.ollama();
    }
  }

  /// Cr√©er un backend selon la configuration
  LLMBackend _createBackend(LLMBackendConfig config) {
    switch (config.type) {
      case LLMBackendType.ollama:
        return OllamaBackend(config);
    }
  }

  /// Construire un contexte √† partir de l'historique de conversation
  ConversationContext _buildContextFromHistory(List<String> history) {
    final messages = <Message>[];

    for (int i = 0; i < history.length; i += 2) {
      if (i + 1 < history.length) {
        // Message utilisateur
        messages.add(Message(
          text: history[i],
          isUser: true,
          timestamp: DateTime.now().subtract(Duration(minutes: history.length - i)),
        ));

        // R√©ponse assistant
        messages.add(Message(
          text: history[i + 1],
          isUser: false,
          timestamp: DateTime.now().subtract(Duration(minutes: history.length - i - 1)),
        ));
      }
    }

    return ConversationContext(
      messages: messages,
      sessionId: _currentContext?.sessionId ?? DateTime.now().millisecondsSinceEpoch.toString(),
      startTime: _currentContext?.startTime ?? DateTime.now(),
    );
  }

  /// Mettre √† jour le contexte de conversation
  void _updateConversationContext(String userMessage, String aiResponse) {
    if (_currentContext == null) {
      startNewConversation();
    }

    // Ajouter le message utilisateur
    final userMsg = Message(
      text: userMessage,
      isUser: true,
      timestamp: DateTime.now(),
    );

    // Ajouter la r√©ponse IA
    final aiMsg = Message(
      text: aiResponse,
      isUser: false,
      timestamp: DateTime.now(),
    );

    _currentContext = _currentContext!.addMessage(userMsg).addMessage(aiMsg);

    // Limiter la taille du contexte si n√©cessaire
    if (_currentContext!.needsSummarization()) {
      _currentContext = _currentContext!.createCondensedContext();
      print('üìù Contexte condens√© pour optimiser les performances');
    }
  }

  // D√©tection de plateforme simplifi√©e
  bool _isMobilePlatform() {
    try {
      return Platform.isAndroid || Platform.isIOS;
    } catch (e) {
      return false;
    }
  }

  bool _isWebPlatform() {
    return identical(0, 0.0); // true sur web
  }

  // === M√©thodes de compatibilit√© avec l'ancienne interface ===

  /// Mode IA r√©elle (pour compatibilit√©)
  void setRealAIMode(bool enabled) {
    print('Mode IA r√©elle: ${enabled ? "activ√©" : "d√©sactiv√©"}');
    // Dans la nouvelle architecture, toujours en mode r√©el
  }
}